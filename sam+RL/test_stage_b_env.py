"""
æµ‹è¯•é˜¶æ®µBç¯å¢ƒï¼šPromptDecisionEnv
éªŒè¯ç¯å¢ƒåŸºç¡€åŠŸèƒ½æ˜¯å¦æ­£å¸¸
"""
import sys
import yaml
import numpy as np

sys.path.insert(0, '/home/ubuntu/sam+RL')

from env.prompt_decision_env import PromptDecisionEnv
from models.sam2_wrapper import SAM2CandidateGenerator
from rewards.reward_functions import RewardCalculator
from utils.data_loader import VesselDataset


def test_basic_functionality():
    """æµ‹è¯•åŸºç¡€åŠŸèƒ½"""
    print("=" * 80)
    print("æµ‹è¯•é˜¶æ®µBç¯å¢ƒåŸºç¡€åŠŸèƒ½")
    print("=" * 80)
    
    # åŠ è½½é…ç½®
    print("\n1. åŠ è½½é…ç½®...")
    config = {
        'sam2': {
            'checkpoint': '/home/ubuntu/sam2.1_hiera_large.pt',
            'model_cfg': 'configs/sam2.1/sam2.1_hiera_l.yaml',
            'device': 'cuda',
            'use_half_precision': True
        },
        'env': {
            'max_steps': 20,
            'grid_size': 32,
            'image_size': [512, 512],
            'max_points': 20
        },
        'reward': {
            'use_gt': True,
            'delta_iou_weight': 10.0,
            'final_iou_weight': 5.0,
            'action_cost': -0.01,
            'iou_decrease_penalty': -0.5
        }
    }
    print("âœ“ é…ç½®åŠ è½½å®Œæˆ")
    
    # åˆå§‹åŒ–ç»„ä»¶
    print("\n2. åˆå§‹åŒ–SAM2...")
    sam_generator = SAM2CandidateGenerator(
        checkpoint=config['sam2']['checkpoint'],
        model_cfg=config['sam2']['model_cfg'],
        device=config['sam2']['device'],
        use_half_precision=config['sam2']['use_half_precision']
    )
    
    print("\n3. åˆå§‹åŒ–å¥–åŠ±è®¡ç®—å™¨...")
    reward_calculator = RewardCalculator(config['reward'])
    print("âœ“ å¥–åŠ±è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # åˆ›å»ºç¯å¢ƒ
    print("\n4. åˆ›å»ºç¯å¢ƒ...")
    env = PromptDecisionEnv(sam_generator, reward_calculator, config)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("\n5. åŠ è½½æµ‹è¯•æ•°æ®...")
    dataset = VesselDataset(
        image_dir="/home/ubuntu/Segment_DATA/orgin_pic",
        mask_dir="/home/ubuntu/Segment_DATA/lab_pic",
        image_size=(512, 512)
    )
    sample = dataset[0]
    print(f"âœ“ æµ‹è¯•æ ·æœ¬: {sample['name']}")
    print(f"  - å›¾åƒå½¢çŠ¶: {sample['image'].shape}")
    print(f"  - æ©è†œå½¢çŠ¶: {sample['mask'].shape}")
    print(f"  - è¡€ç®¡é¢ç§¯: {sample['mask'].sum()} åƒç´  ({sample['mask'].sum()/(512*512)*100:.2f}%)")
    
    # æµ‹è¯•reset
    print("\n6. æµ‹è¯• reset()...")
    obs, info = env.reset(options={'image': sample['image'], 'gt_mask': sample['mask']})
    print(f"âœ“ ResetæˆåŠŸ")
    print(f"  - è§‚å¯Ÿç©ºé—´ç»´åº¦: {obs.shape}")
    print(f"  - åˆå§‹IoU: {info['current_iou']:.4f}")
    print(f"  - æ­¥æ•°: {info['step_count']}")
    
    # æµ‹è¯•åŠ¨ä½œæ‰§è¡Œ
    print("\n7. æµ‹è¯•éšæœºåŠ¨ä½œæ‰§è¡Œ...")
    total_reward = 0.0
    for step in range(5):
        # éšæœºé‡‡æ ·åŠ¨ä½œ
        action = env.action_space.sample()
        
        # æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        
        print(f"\n  æ­¥éª¤ {step + 1}:")
        print(f"    - åŠ¨ä½œ: {action} -> {info['action_name']}")
        if info['action_name'] != 'terminate':
            print(f"    - ä½ç½®: ç½‘æ ¼{info['grid_pos']}") 
        print(f"    - å½“å‰IoU: {info.get('current_iou', 0):.4f}")
        print(f"    - å¥–åŠ±: {reward:.4f}")
        print(f"    - æç¤ºç‚¹æ•°: {info.get('num_points', 0)}")
        
        if terminated or truncated:
            print(f"    - Episodeç»“æŸ")
            break
    
    print(f"\n  æ€»å¥–åŠ±: {total_reward:.4f}")
    
    # æµ‹è¯•åŠ¨ä½œè§£ç 
    print("\n8. æµ‹è¯•åŠ¨ä½œç¼–è§£ç ...")
    test_actions = [
        0,      # add_positive at (0,0)
        1023,   # add_positive at (31,31)
        1024,   # add_negative at (0,0)
        2047,   # add_negative at (31,31)
        2048,   # terminate at (0,0)
        3071    # terminate at (31,31)
    ]
    
    for action in test_actions:
        action_type, grid_x, grid_y = env._decode_action(action)
        action_names = ['positive', 'negative', 'terminate']
        print(f"  åŠ¨ä½œ {action:4d} -> {action_names[action_type]:10s} at grid ({grid_x:2d}, {grid_y:2d})")
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç¯å¢ƒå·¥ä½œæ­£å¸¸")
    print("=" * 80)
    
    return env


def test_full_episode():
    """æµ‹è¯•å®Œæ•´episode"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæ•´Episode")
    print("=" * 80)
    
    # åˆ›å»ºç¯å¢ƒï¼ˆå¤ç”¨å‰é¢çš„é€»è¾‘ï¼‰
    config = {
        'sam2': {
            'checkpoint': '/home/ubuntu/sam2.1_hiera_large.pt',
            'model_cfg': 'configs/sam2.1/sam2.1_hiera_l.yaml',
            'device': 'cuda',
            'use_half_precision': True
        },
        'env': {
            'max_steps': 10,  # å‡å°‘æ­¥æ•°åŠ å¿«æµ‹è¯•
            'grid_size': 32,
            'image_size': [512, 512],
            'max_points': 20
        },
        'reward': {
            'use_gt': True,
            'delta_iou_weight': 10.0,
            'final_iou_weight': 5.0,
            'action_cost': -0.01,
            'iou_decrease_penalty': -0.5
        }
    }
    
    sam_generator = SAM2CandidateGenerator(
        checkpoint=config['sam2']['checkpoint'],
        model_cfg=config['sam2']['model_cfg'],
        device=config['sam2']['device'],
        use_half_precision=config['sam2']['use_half_precision']
    )
    
    reward_calculator = RewardCalculator(config['reward'])
    env = PromptDecisionEnv(sam_generator, reward_calculator, config)
    
    # åŠ è½½æ•°æ®
    dataset = VesselDataset(
        image_dir="/home/ubuntu/Segment_DATA/orgin_pic",
        mask_dir="/home/ubuntu/Segment_DATA/lab_pic",
        image_size=(512, 512)
    )
    
    # è¿è¡Œå®Œæ•´episode
    print("\nè¿è¡Œå®Œæ•´Episode...")
    sample = dataset[0]
    obs, info = env.reset(options={'image': sample['image'], 'gt_mask': sample['mask']})
    
    episode_reward = 0.0
    best_iou = 0.0
    
    for step in range(config['env']['max_steps']):
        # éšæœºç­–ç•¥ï¼ˆåç»­ä¼šç”¨è®­ç»ƒå¥½çš„ç­–ç•¥ï¼‰
        action = env.action_space.sample()
        
        obs, reward, terminated, truncated, info = env.step(action)
        episode_reward += reward
        
        current_iou = info.get('current_iou', 0.0)
        if current_iou > best_iou:
            best_iou = current_iou
        
        print(f"  æ­¥éª¤ {step + 1}: {info['action_name']:10s} | IoU={current_iou:.4f} | å¥–åŠ±={reward:+.4f}")
        
        if terminated or truncated:
            print(f"\n  Episodeç»“æŸ:")
            print(f"    - æœ€ç»ˆIoU: {info.get('final_iou', 0):.4f}")
            print(f"    - æœ€ä½³IoU: {best_iou:.4f}")
            print(f"    - æ€»å¥–åŠ±: {episode_reward:.4f}")
            print(f"    - æ€»æ­¥æ•°: {step + 1}")
            print(f"    - æç¤ºç‚¹æ•°: {info.get('num_points', 0)}")
            break
    
    print("\nâœ… å®Œæ•´Episodeæµ‹è¯•é€šè¿‡ï¼")
    
    return env


if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("é˜¶æ®µBç¯å¢ƒæµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•1ï¼šåŸºç¡€åŠŸèƒ½
    env = test_basic_functionality()
    
    # æµ‹è¯•2ï¼šå®Œæ•´Episode
    env = test_full_episode()
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼é˜¶æ®µBç¯å¢ƒå‡†å¤‡å°±ç»ª")
    print("=" * 80)
    print("\nä¸‹ä¸€æ­¥ï¼š")
    print("1. ä¼˜åŒ–è§‚å¯Ÿç©ºé—´ï¼ˆæ·»åŠ CNNç‰¹å¾æå–ï¼‰")
    print("2. åˆ›å»ºè®­ç»ƒè„šæœ¬")
    print("3. è¿è¡Œåˆæ­¥è®­ç»ƒæµ‹è¯•")
    print()

