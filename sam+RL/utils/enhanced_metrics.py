"""
å¢å¼ºçš„è®­ç»ƒæŒ‡æ ‡è¿½è¸ªç³»ç»Ÿ
è®°å½•è¯¦ç»†çš„è®­ç»ƒæ›²çº¿ã€IoUè¶‹åŠ¿ã€åŠ¨ä½œåˆ†å¸ƒç­‰
"""
import numpy as np
from collections import defaultdict, deque
from typing import Dict, List, Any
import json
from pathlib import Path


class EnhancedMetricsTracker:
    """
    å¢å¼ºçš„æŒ‡æ ‡è¿½è¸ªå™¨
    """
    
    def __init__(self, window_size: int = 100, save_dir: str = "./logs"):
        """
        Args:
            window_size: ç§»åŠ¨å¹³å‡çª—å£å¤§å°
            save_dir: ä¿å­˜ç›®å½•
        """
        self.window_size = window_size
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # åŸºæœ¬ç»Ÿè®¡
        self.total_steps = 0
        self.total_episodes = 0
        
        # å¥–åŠ±è¿½è¸ª
        self.episode_rewards = []
        self.step_rewards = []
        self.reward_components = defaultdict(list)  # iou, cldice, etc.
        
        # IoUè¿½è¸ª
        self.episode_ious = []
        self.step_ious = []
        self.best_iou = 0.0
        self.best_iou_episode = 0
        
        # åŠ¨ä½œç»Ÿè®¡
        self.action_counts = defaultdict(int)
        self.action_history = []
        
        # Episodeç»Ÿè®¡
        self.episode_lengths = []
        self.episode_final_ious = []
        self.episode_final_areas = []  # æœ€ç»ˆæ©è†œé¢ç§¯
        
        # ç§»åŠ¨å¹³å‡
        self.reward_window = deque(maxlen=window_size)
        self.iou_window = deque(maxlen=window_size)
        
        # å½“å‰episodeç¼“å­˜
        self.current_episode = {
            'rewards': [],
            'ious': [],
            'actions': [],
            'reward_components': defaultdict(list)
        }
        
        # SAM2ç»Ÿè®¡
        self.sam2_inference_times = []
        self.candidates_generated = []
        
        # è®­ç»ƒé˜¶æ®µç»Ÿè®¡
        self.phase_stats = {
            'early': {'episodes': 0, 'avg_reward': 0, 'avg_iou': 0},  # å‰1000æ­¥
            'mid': {'episodes': 0, 'avg_reward': 0, 'avg_iou': 0},    # 1000-5000æ­¥
            'late': {'episodes': 0, 'avg_reward': 0, 'avg_iou': 0}    # 5000+æ­¥
        }
    
    def log_step(self, 
                 reward: float, 
                 action: int, 
                 iou: float = None,
                 reward_components: Dict[str, float] = None,
                 **kwargs):
        """
        è®°å½•å•æ­¥ä¿¡æ¯
        
        Args:
            reward: æ­¥å¥–åŠ±
            action: åŠ¨ä½œID
            iou: å½“å‰IoUï¼ˆå¦‚æœæœ‰GTï¼‰
            reward_components: å¥–åŠ±å„ç»„æˆéƒ¨åˆ†
            **kwargs: å…¶ä»–ä¿¡æ¯
        """
        self.total_steps += 1
        self.step_rewards.append(reward)
        
        # è®°å½•åŠ¨ä½œ
        self.action_counts[action] += 1
        self.action_history.append(action)
        self.current_episode['actions'].append(action)
        self.current_episode['rewards'].append(reward)
        
        # è®°å½•IoU
        if iou is not None:
            self.step_ious.append(iou)
            self.current_episode['ious'].append(iou)
            self.iou_window.append(iou)
        
        # è®°å½•å¥–åŠ±ç»„ä»¶
        if reward_components:
            for key, value in reward_components.items():
                self.reward_components[key].append(value)
                self.current_episode['reward_components'][key].append(value)
        
        # è®°å½•å…¶ä»–ä¿¡æ¯
        if 'sam2_time' in kwargs:
            self.sam2_inference_times.append(kwargs['sam2_time'])
        if 'num_candidates' in kwargs:
            self.candidates_generated.append(kwargs['num_candidates'])
    
    def log_episode_end(self, final_iou: float = None, final_area: int = None):
        """
        è®°å½•episodeç»“æŸ
        
        Args:
            final_iou: æœ€ç»ˆIoU
            final_area: æœ€ç»ˆæ©è†œé¢ç§¯
        """
        self.total_episodes += 1
        
        # è®¡ç®—episodeæ€»å¥–åŠ±
        episode_reward = sum(self.current_episode['rewards'])
        self.episode_rewards.append(episode_reward)
        self.reward_window.append(episode_reward)
        
        # è®°å½•episodeé•¿åº¦
        episode_length = len(self.current_episode['rewards'])
        self.episode_lengths.append(episode_length)
        
        # è®°å½•æœ€ç»ˆIoU
        if final_iou is not None:
            self.episode_final_ious.append(final_iou)
            self.episode_ious.append(final_iou)
            
            # æ›´æ–°æœ€ä½³IoU
            if final_iou > self.best_iou:
                self.best_iou = final_iou
                self.best_iou_episode = self.total_episodes
        
        # è®°å½•æœ€ç»ˆé¢ç§¯
        if final_area is not None:
            self.episode_final_areas.append(final_area)
        
        # æ›´æ–°è®­ç»ƒé˜¶æ®µç»Ÿè®¡
        self._update_phase_stats(episode_reward, final_iou)
        
        # æ¸…ç©ºå½“å‰episodeç¼“å­˜
        self.current_episode = {
            'rewards': [],
            'ious': [],
            'actions': [],
            'reward_components': defaultdict(list)
        }
    
    def _update_phase_stats(self, reward: float, iou: float = None):
        """æ›´æ–°è®­ç»ƒé˜¶æ®µç»Ÿè®¡"""
        if self.total_steps < 1000:
            phase = 'early'
        elif self.total_steps < 5000:
            phase = 'mid'
        else:
            phase = 'late'
        
        stats = self.phase_stats[phase]
        n = stats['episodes']
        stats['episodes'] += 1
        stats['avg_reward'] = (stats['avg_reward'] * n + reward) / (n + 1)
        if iou is not None:
            stats['avg_iou'] = (stats['avg_iou'] * n + iou) / (n + 1)
    
    def get_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡æ‘˜è¦
        
        Args:
            last_n: æœ€è¿‘Nä¸ªepisodes
            
        Returns:
            summary: ç»Ÿè®¡æ‘˜è¦å­—å…¸
        """
        summary = {}
        
        # åŸºæœ¬ç»Ÿè®¡
        summary['total_steps'] = self.total_steps
        summary['total_episodes'] = self.total_episodes
        
        # å¥–åŠ±ç»Ÿè®¡
        if self.episode_rewards:
            recent_rewards = self.episode_rewards[-last_n:]
            summary['avg_episode_reward'] = np.mean(recent_rewards)
            summary['std_episode_reward'] = np.std(recent_rewards)
            summary['min_episode_reward'] = np.min(recent_rewards)
            summary['max_episode_reward'] = np.max(recent_rewards)
            summary['moving_avg_reward'] = np.mean(list(self.reward_window)) if self.reward_window else 0
        
        # IoUç»Ÿè®¡
        if self.episode_final_ious:
            recent_ious = self.episode_final_ious[-last_n:]
            summary['avg_final_iou'] = np.mean(recent_ious)
            summary['std_final_iou'] = np.std(recent_ious)
            summary['min_final_iou'] = np.min(recent_ious)
            summary['max_final_iou'] = np.max(recent_ious)
            summary['best_iou'] = self.best_iou
            summary['best_iou_episode'] = self.best_iou_episode
            summary['moving_avg_iou'] = np.mean(list(self.iou_window)) if self.iou_window else 0
        
        # åŠ¨ä½œç»Ÿè®¡
        total_actions = sum(self.action_counts.values())
        if total_actions > 0:
            summary['action_distribution'] = {
                k: v / total_actions for k, v in self.action_counts.items()
            }
            summary['action_counts'] = dict(self.action_counts)
        
        # Episodeé•¿åº¦ç»Ÿè®¡
        if self.episode_lengths:
            recent_lengths = self.episode_lengths[-last_n:]
            summary['avg_episode_length'] = np.mean(recent_lengths)
            summary['std_episode_length'] = np.std(recent_lengths)
        
        # æ©è†œé¢ç§¯ç»Ÿè®¡
        if self.episode_final_areas:
            recent_areas = self.episode_final_areas[-last_n:]
            summary['avg_final_area'] = np.mean(recent_areas)
            summary['std_final_area'] = np.std(recent_areas)
        
        # SAM2ç»Ÿè®¡
        if self.sam2_inference_times:
            recent_times = [t for t in self.sam2_inference_times[-last_n:] if t is not None]
            if recent_times:
                summary['avg_sam2_time'] = np.mean(recent_times)
        
        if self.candidates_generated:
            recent_candidates = [c for c in self.candidates_generated[-last_n:] if c is not None]
            if recent_candidates:
                summary['avg_candidates'] = np.mean(recent_candidates)
        
        # è®­ç»ƒé˜¶æ®µç»Ÿè®¡
        summary['phase_stats'] = self.phase_stats
        
        # å¥–åŠ±ç»„ä»¶ç»Ÿè®¡
        if self.reward_components:
            summary['reward_components'] = {}
            for key, values in self.reward_components.items():
                recent_values = values[-last_n*10:]  # æ¯ä¸ªepisodeå¯èƒ½æœ‰å¤šæ­¥
                if recent_values:
                    summary['reward_components'][key] = {
                        'mean': float(np.mean(recent_values)),
                        'std': float(np.std(recent_values))
                    }
        
        return summary
    
    def print_summary(self, last_n: int = 100):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        summary = self.get_summary(last_n)
        
        print("\n" + "=" * 80)
        print(f"è®­ç»ƒç»Ÿè®¡æ‘˜è¦ (æœ€è¿‘{last_n}ä¸ªepisodes)")
        print("=" * 80)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"  - æ€»æ­¥æ•°: {summary['total_steps']}")
        print(f"  - æ€»Episodes: {summary['total_episodes']}")
        
        # å¥–åŠ±
        if 'avg_episode_reward' in summary:
            print(f"\nğŸ’° å¥–åŠ±:")
            print(f"  - å¹³å‡å¥–åŠ±: {summary['avg_episode_reward']:.4f} Â± {summary['std_episode_reward']:.4f}")
            print(f"  - å¥–åŠ±èŒƒå›´: [{summary['min_episode_reward']:.4f}, {summary['max_episode_reward']:.4f}]")
            print(f"  - ç§»åŠ¨å¹³å‡: {summary['moving_avg_reward']:.4f}")
        
        # IoU
        if 'avg_final_iou' in summary:
            print(f"\nğŸ¯ IoU:")
            print(f"  - å¹³å‡IoU: {summary['avg_final_iou']:.4f} Â± {summary['std_final_iou']:.4f}")
            print(f"  - IoUèŒƒå›´: [{summary['min_final_iou']:.4f}, {summary['max_final_iou']:.4f}]")
            print(f"  - æœ€ä½³IoU: {summary['best_iou']:.4f} (Episode {summary['best_iou_episode']})")
            print(f"  - ç§»åŠ¨å¹³å‡: {summary['moving_avg_iou']:.4f}")
        
        # åŠ¨ä½œ
        if 'action_distribution' in summary:
            print(f"\nğŸ¬ åŠ¨ä½œåˆ†å¸ƒ:")
            action_names = {0: 'select', 1: 'sample', 2: 'merge', 3: 'terminate'}
            for action_id, ratio in summary['action_distribution'].items():
                action_name = action_names.get(action_id, f'action_{action_id}')
                count = summary['action_counts'][action_id]
                print(f"  - {action_name}: {ratio*100:.1f}% ({count}æ¬¡)")
        
        # Episodeé•¿åº¦
        if 'avg_episode_length' in summary:
            print(f"\nğŸ“ Episodeé•¿åº¦:")
            print(f"  - å¹³å‡: {summary['avg_episode_length']:.2f} Â± {summary['std_episode_length']:.2f}")
        
        # æ©è†œé¢ç§¯
        if 'avg_final_area' in summary:
            print(f"\nğŸ“ æœ€ç»ˆæ©è†œé¢ç§¯:")
            print(f"  - å¹³å‡: {summary['avg_final_area']:.0f} Â± {summary['std_final_area']:.0f} åƒç´ ")
            print(f"  - å æ¯”: {summary['avg_final_area']/(512*512)*100:.2f}%")
        
        # è®­ç»ƒé˜¶æ®µ
        print(f"\nğŸ“ˆ è®­ç»ƒé˜¶æ®µ:")
        for phase, stats in summary['phase_stats'].items():
            if stats['episodes'] > 0:
                print(f"  - {phase}: {stats['episodes']}ä¸ªepisodes, "
                      f"å¹³å‡å¥–åŠ±={stats['avg_reward']:.4f}, "
                      f"å¹³å‡IoU={stats['avg_iou']:.4f}")
        
        # å¥–åŠ±ç»„ä»¶
        if 'reward_components' in summary and summary['reward_components']:
            print(f"\nğŸ” å¥–åŠ±ç»„ä»¶:")
            for key, stats in summary['reward_components'].items():
                print(f"  - {key}: {stats['mean']:.4f} Â± {stats['std']:.4f}")
        
        print("=" * 80)
    
    def save(self, filename: str = "metrics.json"):
        """ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        filepath = self.save_dir / filename
        
        # è½¬æ¢æ‰€æœ‰numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        def convert_to_native(obj):
            """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
            if isinstance(obj, dict):
                return {str(k): convert_to_native(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [convert_to_native(item) for item in obj]
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return obj
        
        data = {
            'total_steps': int(self.total_steps),
            'total_episodes': int(self.total_episodes),
            'episode_rewards': [float(x) for x in self.episode_rewards],
            'episode_ious': [float(x) for x in self.episode_ious],
            'episode_lengths': [int(x) for x in self.episode_lengths],
            'episode_final_ious': [float(x) for x in self.episode_final_ious],
            'episode_final_areas': [int(x) for x in self.episode_final_areas],
            'action_counts': {str(k): int(v) for k, v in self.action_counts.items()},
            'best_iou': float(self.best_iou),
            'best_iou_episode': int(self.best_iou_episode),
            'phase_stats': convert_to_native(self.phase_stats),
            'reward_components': {k: [float(x) for x in v] for k, v in self.reward_components.items()}
        }
        
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)
        
        print(f"âœ“ æŒ‡æ ‡å·²ä¿å­˜åˆ°: {filepath}")
    
    def load(self, filename: str = "metrics.json"):
        """ä»æ–‡ä»¶åŠ è½½æŒ‡æ ‡"""
        filepath = self.save_dir / filename
        
        if not filepath.exists():
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return
        
        with open(filepath, 'r') as f:
            data = json.load(f)
        
        self.total_steps = data['total_steps']
        self.total_episodes = data['total_episodes']
        self.episode_rewards = data['episode_rewards']
        self.episode_ious = data['episode_ious']
        self.episode_lengths = data['episode_lengths']
        self.episode_final_ious = data['episode_final_ious']
        self.episode_final_areas = data['episode_final_areas']
        self.action_counts = defaultdict(int, data['action_counts'])
        self.best_iou = data['best_iou']
        self.best_iou_episode = data['best_iou_episode']
        self.phase_stats = data['phase_stats']
        self.reward_components = defaultdict(list, data['reward_components'])
        
        print(f"âœ“ æŒ‡æ ‡å·²ä»æ–‡ä»¶åŠ è½½: {filepath}")

