#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆè¯„ä¼°è„šæœ¬
æ·»åŠ æ›´å¤šè¯¦ç»†æŒ‡æ ‡ç”¨äºæ·±å…¥åˆ†ææ¨¡å‹é—®é¢˜
"""
import argparse
import numpy as np
import nibabel as nib
from pathlib import Path
from tqdm import tqdm
import json

def calculate_metrics_enhanced(pred, gt):
    """
    è®¡ç®—å¢å¼ºçš„åˆ†å‰²æŒ‡æ ‡
    åŒ…æ‹¬åŸºç¡€æŒ‡æ ‡ã€ç»Ÿè®¡åˆ†æã€è¾¹ç•ŒæŒ‡æ ‡ç­‰
    """
    # ç¡®ä¿æ˜¯äºŒå€¼æ•°æ®
    pred = (pred > 0.5).astype(np.uint8)
    gt = gt.astype(np.uint8)
    
    # åŸºæœ¬ç»Ÿè®¡
    TP = np.sum((pred == 1) & (gt == 1))
    TN = np.sum((pred == 0) & (gt == 0))
    FP = np.sum((pred == 1) & (gt == 0))
    FN = np.sum((pred == 0) & (gt == 1))
    
    total_pixels = pred.size
    epsilon = 1e-7
    
    metrics = {}
    
    # === åŸºç¡€æŒ‡æ ‡ ===
    metrics['accuracy'] = (TP + TN) / (total_pixels + epsilon)
    metrics['precision'] = TP / (TP + FP + epsilon)
    metrics['sensitivity'] = TP / (TP + FN + epsilon)
    metrics['specificity'] = TN / (TN + FP + epsilon)
    metrics['f1'] = 2 * TP / (2 * TP + FP + FN + epsilon)
    metrics['iou'] = TP / (TP + FP + FN + epsilon)
    metrics['dice'] = 2 * TP / (2 * TP + FP + FN + epsilon)
    
    # === ç»Ÿè®¡æŒ‡æ ‡ ===
    metrics['TP'] = int(TP)
    metrics['TN'] = int(TN)
    metrics['FP'] = int(FP)
    metrics['FN'] = int(FN)
    metrics['total_pixels'] = int(total_pixels)
    
    # å‰æ™¯/èƒŒæ™¯æ¯”ä¾‹
    gt_fg_pixels = int(np.sum(gt))
    pred_fg_pixels = int(np.sum(pred))
    
    metrics['gt_foreground_ratio'] = gt_fg_pixels / total_pixels
    metrics['pred_foreground_ratio'] = pred_fg_pixels / total_pixels
    metrics['gt_foreground_pixels'] = gt_fg_pixels
    metrics['pred_foreground_pixels'] = pred_fg_pixels
    
    # === é”™è¯¯åˆ†æ ===
    # False Positive Rate (å‡é˜³æ€§ç‡)
    metrics['fpr'] = FP / (FP + TN + epsilon)
    
    # False Negative Rate (å‡é˜´æ€§ç‡/æ¼æ£€ç‡)
    metrics['fnr'] = FN / (FN + TP + epsilon)
    
    # è¿‡åº¦é¢„æµ‹æ¯”ä¾‹ (é¢„æµ‹æ¯”GTå¤šå¤šå°‘)
    metrics['over_prediction_ratio'] = (pred_fg_pixels - gt_fg_pixels) / (gt_fg_pixels + epsilon)
    
    # é¢„æµ‹è¦†ç›–ç‡ (é¢„æµ‹äº†å¤šå°‘çœŸå®å‰æ™¯)
    metrics['coverage'] = TP / (gt_fg_pixels + epsilon)
    
    # é¢„æµ‹çº¯åº¦ (é¢„æµ‹ä¸­æœ‰å¤šå°‘æ˜¯æ­£ç¡®çš„)
    metrics['purity'] = TP / (pred_fg_pixels + epsilon) if pred_fg_pixels > 0 else 0.0
    
    # === åƒç´ çº§ç»Ÿè®¡ ===
    # è®¡ç®—é¢„æµ‹åˆ†å¸ƒçš„åå·®
    if gt_fg_pixels > 0:
        # è¿‡åº¦é¢„æµ‹å€æ•°
        metrics['fg_prediction_fold'] = pred_fg_pixels / gt_fg_pixels
    else:
        metrics['fg_prediction_fold'] = float('inf') if pred_fg_pixels > 0 else 1.0
    
    # === åˆ†ç±»è´¨é‡ ===
    # Matthews Correlation Coefficient (MCC)
    mcc_num = (TP * TN - FP * FN)
    mcc_den = np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))
    metrics['mcc'] = mcc_num / (mcc_den + epsilon)
    
    # Balanced Accuracy
    metrics['balanced_accuracy'] = (metrics['sensitivity'] + metrics['specificity']) / 2
    
    # === ä½“ç§¯ç›¸å…³ ===
    metrics['volume_similarity'] = 1 - abs(pred_fg_pixels - gt_fg_pixels) / (pred_fg_pixels + gt_fg_pixels + epsilon)
    
    return metrics


def analyze_per_slice(pred, gt):
    """é€sliceåˆ†æï¼ˆå¦‚æœæ˜¯3Dæ•°æ®ï¼‰"""
    if len(pred.shape) == 3:
        slice_metrics = []
        for i in range(pred.shape[2]):
            pred_slice = pred[:, :, i]
            gt_slice = gt[:, :, i]
            
            # è·³è¿‡ç©ºslice
            if gt_slice.sum() == 0 and pred_slice.sum() == 0:
                continue
            
            metrics = calculate_metrics_enhanced(pred_slice, gt_slice)
            metrics['slice_idx'] = i
            slice_metrics.append(metrics)
        
        return slice_metrics
    return []


def main():
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆè¯„ä¼°å·¥å…·')
    parser.add_argument('--reward-ds-path', type=str,
                       default='/home/ubuntu/my_rl4seg3d_logs/3d_test/rewardDS',
                       help='RewardDSç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', type=str,
                       default='/home/ubuntu/evaluation_enhanced',
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--analyze-slices', action='store_true',
                       help='æ˜¯å¦è¿›è¡Œé€sliceåˆ†æ')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print(" "*25 + "å¢å¼ºç‰ˆåˆ†å‰²è¯„ä¼°å·¥å…·")
    print("="*80)
    
    reward_ds_path = Path(args.reward_ds_path)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    pred_dir = reward_ds_path / 'pred'
    gt_dir = reward_ds_path / 'gt'
    images_dir = reward_ds_path / 'images'
    
    if not pred_dir.exists() or not gt_dir.exists():
        print(f"\nâŒ é”™è¯¯: predæˆ–gtç›®å½•ä¸å­˜åœ¨")
        return 1
    
    pred_files = sorted(list(pred_dir.glob('*.nii.gz')))
    
    if not pred_files:
        print(f"\nâŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°é¢„æµ‹æ–‡ä»¶")
        return 1
    
    print(f"\nğŸ“‚ æ•°æ®è·¯å¾„: {reward_ds_path}")
    print(f"ğŸ“Š æ‰¾åˆ° {len(pred_files)} ä¸ªæ ·æœ¬")
    print(f"ğŸ’¾ ç»“æœä¿å­˜åˆ°: {output_dir}")
    
    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
    all_metrics = []
    all_slice_metrics = []
    
    print("\n" + "="*80)
    print("å¼€å§‹è¯¦ç»†è¯„ä¼°...")
    print("="*80 + "\n")
    
    for pred_file in tqdm(pred_files, desc="è¯„ä¼°è¿›åº¦"):
        try:
            # åŠ è½½æ•°æ®
            pred = nib.load(pred_file).get_fdata()
            gt_file = gt_dir / pred_file.name
            
            if not gt_file.exists():
                continue
            
            gt = nib.load(gt_file).get_fdata()
            
            if pred.shape != gt.shape:
                print(f"âš ï¸  å½¢çŠ¶ä¸åŒ¹é…: {pred_file.name}")
                continue
            
            # è®¡ç®—æ•´ä½“æŒ‡æ ‡
            metrics = calculate_metrics_enhanced(pred, gt)
            metrics['filename'] = pred_file.name
            metrics['shape'] = pred.shape
            
            all_metrics.append(metrics)
            
            # é€sliceåˆ†æ
            if args.analyze_slices:
                slice_metrics = analyze_per_slice(pred, gt)
                for sm in slice_metrics:
                    sm['filename'] = pred_file.name
                all_slice_metrics.extend(slice_metrics)
        
        except Exception as e:
            print(f"âŒ é”™è¯¯ {pred_file.name}: {e}")
            continue
    
    if not all_metrics:
        print("\nâŒ æ²¡æœ‰æˆåŠŸè¯„ä¼°ä»»ä½•æ ·æœ¬")
        return 1
    
    # === ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š ===
    print("\n" + "="*80)
    print(" "*25 + "è¯¦ç»†è¯„ä¼°ç»“æœ")
    print("="*80)
    
    # 1. åŸºç¡€æŒ‡æ ‡ç»Ÿè®¡
    print("\nğŸ“Š åŸºç¡€åˆ†å‰²æŒ‡æ ‡:")
    print("-"*80)
    metric_names = ['dice', 'iou', 'accuracy', 'precision', 'sensitivity', 'specificity', 'f1']
    print(f"{'æŒ‡æ ‡':<15} {'å¹³å‡å€¼':>10} {'æ ‡å‡†å·®':>10} {'æœ€å°å€¼':>10} {'æœ€å¤§å€¼':>10}")
    print("-"*80)
    
    for metric in metric_names:
        values = [m[metric] for m in all_metrics]
        print(f"{metric.upper():<15} {np.mean(values):10.4f} {np.std(values):10.4f} "
              f"{np.min(values):10.4f} {np.max(values):10.4f}")
    
    # 2. é”™è¯¯åˆ†æ
    print("\nâŒ é”™è¯¯åˆ†æ:")
    print("-"*80)
    fpr_values = [m['fpr'] for m in all_metrics]
    fnr_values = [m['fnr'] for m in all_metrics]
    print(f"å‡é˜³æ€§ç‡ (FPR):        {np.mean(fpr_values):.4f} Â± {np.std(fpr_values):.4f}")
    print(f"å‡é˜´æ€§ç‡ (FNR):        {np.mean(fnr_values):.4f} Â± {np.std(fnr_values):.4f}")
    
    # 3. å‰æ™¯æ¯”ä¾‹åˆ†æ
    print("\nğŸ¯ å‰æ™¯æ¯”ä¾‹åˆ†æ:")
    print("-"*80)
    gt_fg_ratios = [m['gt_foreground_ratio'] for m in all_metrics]
    pred_fg_ratios = [m['pred_foreground_ratio'] for m in all_metrics]
    over_pred_ratios = [m['over_prediction_ratio'] for m in all_metrics]
    fg_folds = [m['fg_prediction_fold'] for m in all_metrics if m['fg_prediction_fold'] != float('inf')]
    
    print(f"GTå‰æ™¯æ¯”ä¾‹:            {np.mean(gt_fg_ratios)*100:.2f}% Â± {np.std(gt_fg_ratios)*100:.2f}%")
    print(f"é¢„æµ‹å‰æ™¯æ¯”ä¾‹:          {np.mean(pred_fg_ratios)*100:.2f}% Â± {np.std(pred_fg_ratios)*100:.2f}%")
    print(f"è¿‡åº¦é¢„æµ‹æ¯”ä¾‹:          {np.mean(over_pred_ratios)*100:.1f}% (ç›¸å¯¹GT)")
    print(f"é¢„æµ‹å€æ•°:              {np.mean(fg_folds):.2f}x (é¢„æµ‹æ˜¯GTçš„å‡ å€)")
    
    # 4. é¢„æµ‹è´¨é‡åˆ†æ
    print("\nâœ… é¢„æµ‹è´¨é‡åˆ†æ:")
    print("-"*80)
    coverages = [m['coverage'] for m in all_metrics]
    purities = [m['purity'] for m in all_metrics]
    print(f"è¦†ç›–ç‡ (æ•è·äº†å¤šå°‘GT):  {np.mean(coverages)*100:.2f}%")
    print(f"çº¯åº¦ (é¢„æµ‹ä¸­æ­£ç¡®æ¯”ä¾‹):  {np.mean(purities)*100:.2f}%")
    
    # 5. æ··æ·†çŸ©é˜µç»Ÿè®¡
    print("\nğŸ“ˆ æ··æ·†çŸ©é˜µç»Ÿè®¡ (æ€»å’Œ):")
    print("-"*80)
    total_TP = sum(m['TP'] for m in all_metrics)
    total_FP = sum(m['FP'] for m in all_metrics)
    total_FN = sum(m['FN'] for m in all_metrics)
    total_TN = sum(m['TN'] for m in all_metrics)
    total_all = total_TP + total_FP + total_FN + total_TN
    
    print(f"True Positive (TP):    {total_TP:,} ({total_TP/total_all*100:.2f}%)")
    print(f"False Positive (FP):   {total_FP:,} ({total_FP/total_all*100:.2f}%)")
    print(f"False Negative (FN):   {total_FN:,} ({total_FN/total_all*100:.2f}%)")
    print(f"True Negative (TN):    {total_TN:,} ({total_TN/total_all*100:.2f}%)")
    
    # 6. æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†æŒ‡æ ‡
    print("\nğŸ“‹ æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†æŒ‡æ ‡:")
    print("-"*80)
    print(f"{'æ–‡ä»¶å':<35} {'Dice':>7} {'å‰æ™¯æ¯”(GT)':>12} {'å‰æ™¯æ¯”(Pred)':>14} {'è¿‡åº¦é¢„æµ‹':>10}")
    print("-"*80)
    for m in all_metrics:
        print(f"{m['filename']:<35} {m['dice']:7.4f} "
              f"{m['gt_foreground_ratio']*100:10.2f}% "
              f"{m['pred_foreground_ratio']*100:12.2f}% "
              f"{m['over_prediction_ratio']*100:9.1f}%")
    
    # === ä¿å­˜è¯¦ç»†ç»“æœ ===
    # ä¿å­˜JSON
    json_path = output_dir / 'evaluation_detailed.json'
    with open(json_path, 'w') as f:
        json.dump({
            'summary': {
                'num_samples': len(all_metrics),
                'mean_dice': float(np.mean([m['dice'] for m in all_metrics])),
                'mean_iou': float(np.mean([m['iou'] for m in all_metrics])),
                'mean_precision': float(np.mean([m['precision'] for m in all_metrics])),
                'mean_sensitivity': float(np.mean([m['sensitivity'] for m in all_metrics])),
                'mean_gt_fg_ratio': float(np.mean(gt_fg_ratios)),
                'mean_pred_fg_ratio': float(np.mean(pred_fg_ratios)),
                'mean_over_prediction': float(np.mean(over_pred_ratios)),
            },
            'per_sample_metrics': all_metrics
        }, f, indent=2, default=str)
    
    # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
    txt_path = output_dir / 'evaluation_report.txt'
    with open(txt_path, 'w') as f:
        f.write("="*80 + "\n")
        f.write(" "*25 + "å¢å¼ºç‰ˆè¯„ä¼°æŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")
        f.write(f"æ ·æœ¬æ•°: {len(all_metrics)}\n")
        f.write(f"æ•°æ®è·¯å¾„: {reward_ds_path}\n\n")
        
        f.write("åŸºç¡€æŒ‡æ ‡:\n")
        for metric in metric_names:
            values = [m[metric] for m in all_metrics]
            f.write(f"  {metric.upper()}: {np.mean(values):.4f} Â± {np.std(values):.4f}\n")
        
        f.write(f"\nå‰æ™¯æ¯”ä¾‹:\n")
        f.write(f"  GTå‰æ™¯: {np.mean(gt_fg_ratios)*100:.2f}%\n")
        f.write(f"  é¢„æµ‹å‰æ™¯: {np.mean(pred_fg_ratios)*100:.2f}%\n")
        f.write(f"  è¿‡åº¦é¢„æµ‹: {np.mean(over_pred_ratios)*100:.1f}%\n")
        
        f.write(f"\næ··æ·†çŸ©é˜µ:\n")
        f.write(f"  TP: {total_TP:,}, FP: {total_FP:,}\n")
        f.write(f"  FN: {total_FN:,}, TN: {total_TN:,}\n")
    
    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜:")
    print(f"   JSON: {json_path}")
    print(f"   æ–‡æœ¬: {txt_path}")
    
    # === è¯Šæ–­å»ºè®® ===
    print("\n" + "="*80)
    print(" "*25 + "ğŸ” è¯Šæ–­å’Œå»ºè®®")
    print("="*80)
    
    avg_dice = np.mean([m['dice'] for m in all_metrics])
    avg_precision = np.mean([m['precision'] for m in all_metrics])
    avg_sensitivity = np.mean([m['sensitivity'] for m in all_metrics])
    avg_over_pred = np.mean(over_pred_ratios)
    
    issues = []
    
    if avg_dice < 0.3:
        issues.append(("ğŸš¨ ä¸¥é‡", f"Dice Scoreè¿‡ä½ ({avg_dice:.1%})"))
    elif avg_dice < 0.6:
        issues.append(("âš ï¸  ä¸­ç­‰", f"Dice Scoreåä½ ({avg_dice:.1%})"))
    
    if avg_precision < 0.1:
        issues.append(("ğŸš¨ ä¸¥é‡", f"Precisionæä½ ({avg_precision:.1%}) - å¤§é‡å‡é˜³æ€§"))
    elif avg_precision < 0.5:
        issues.append(("âš ï¸  ä¸­ç­‰", f"Precisionåä½ ({avg_precision:.1%}) - è¾ƒå¤šå‡é˜³æ€§"))
    
    if avg_over_pred > 10:
        issues.append(("ğŸš¨ ä¸¥é‡", f"ä¸¥é‡è¿‡åº¦é¢„æµ‹ (é¢„æµ‹æ˜¯GTçš„{avg_over_pred:.0f}å€)"))
    elif avg_over_pred > 3:
        issues.append(("âš ï¸  ä¸­ç­‰", f"æ˜æ˜¾è¿‡åº¦é¢„æµ‹ (é¢„æµ‹æ˜¯GTçš„{avg_over_pred:.1f}å€)"))
    
    if avg_sensitivity < 0.5:
        issues.append(("âš ï¸  æ³¨æ„", f"Sensitivityåä½ ({avg_sensitivity:.1%}) - æ¼æ£€è¾ƒå¤š"))
    
    if issues:
        print("\nå‘ç°çš„é—®é¢˜:")
        for severity, issue in issues:
            print(f"  {severity}: {issue}")
    else:
        print("\nâœ… æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼")
    
    print("\n" + "="*80)
    
    return 0


if __name__ == '__main__':
    import sys
    sys.exit(main())
