#!/usr/bin/env python3
"""
ç›´æ¥è¯„ä¼°é¢„æµ‹ç»“æœï¼ˆä»rewardDSç›®å½•ï¼‰
è®¡ç®—æ‰€æœ‰åˆ†å‰²æŒ‡æ ‡ï¼šAccuracy, Precision, Sensitivity, Specificity, IoU, F1, Dice
"""
import argparse
import numpy as np
import nibabel as nib
from pathlib import Path
from tqdm import tqdm

def calculate_metrics(pred, gt):
    """
    è®¡ç®—å®Œæ•´çš„åˆ†å‰²æŒ‡æ ‡
    
    å‚æ•°:
        pred: é¢„æµ‹ç»“æœ (numpy array, äºŒå€¼0/1)
        gt: Ground Truth (numpy array, äºŒå€¼0/1)
    
    è¿”å›:
        dict: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
    """
    # ç¡®ä¿æ˜¯äºŒå€¼æ•°æ®
    pred = (pred > 0.5).astype(np.float32) if pred.max() > 1 else pred.astype(np.float32)
    gt = gt.astype(np.float32)
    
    # åŸºæœ¬ç»Ÿè®¡
    TP = np.sum((pred == 1) & (gt == 1))  # True Positive
    TN = np.sum((pred == 0) & (gt == 0))  # True Negative  
    FP = np.sum((pred == 1) & (gt == 0))  # False Positive
    FN = np.sum((pred == 0) & (gt == 1))  # False Negative
    
    epsilon = 1e-7  # é¿å…é™¤é›¶
    
    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    metrics = {}
    
    # 1. Accuracy (ACC - å‡†ç¡®ç‡)
    metrics['accuracy'] = (TP + TN) / (TP + TN + FP + FN + epsilon)
    
    # 2. Precision (PRE - ç²¾ç¡®ç‡/æŸ¥å‡†ç‡)
    metrics['precision'] = TP / (TP + FP + epsilon)
    
    # 3. Sensitivity/Recall (SEN - çµæ•åº¦/å¬å›ç‡)
    metrics['sensitivity'] = TP / (TP + FN + epsilon)
    metrics['recall'] = metrics['sensitivity']  # åŒä¹‰è¯
    
    # 4. Specificity (SPE - ç‰¹å¼‚åº¦)
    metrics['specificity'] = TN / (TN + FP + epsilon)
    
    # 5. F1 Score (F1åˆ†æ•°)
    metrics['f1'] = 2 * TP / (2 * TP + FP + FN + epsilon)
    
    # 6. IoU/Jaccard (IOU - äº¤å¹¶æ¯”)
    intersection = TP
    union = TP + FP + FN
    metrics['iou'] = intersection / (union + epsilon)
    metrics['jaccard'] = metrics['iou']  # åŒä¹‰è¯
    
    # 7. Dice Coefficient (DSC - Diceç³»æ•°)
    metrics['dice'] = 2 * TP / (2 * TP + FP + FN + epsilon)
    
    # åŸºæœ¬ç»Ÿè®¡
    metrics['TP'] = TP
    metrics['TN'] = TN
    metrics['FP'] = FP
    metrics['FN'] = FN
    
    return metrics

def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°é¢„æµ‹ç»“æœï¼ˆrewardDSï¼‰')
    parser.add_argument('--rewardds-dir', default='/home/ubuntu/my_rl4seg3d_logs/3d_test/rewardDS',
                       help='rewardDSç›®å½•è·¯å¾„')
    parser.add_argument('--output', default='/home/ubuntu/evaluation_results.txt',
                       help='ä¿å­˜ç»“æœåˆ°æ–‡ä»¶')
    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºæ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯')
    args = parser.parse_args()
    
    print("="*80)
    print(" "*25 + "åˆ†å‰²æ¨¡å‹è¯„ä¼°å·¥å…·")
    print("="*80)
    
    rewardds_path = Path(args.rewardds_dir)
    
    if not rewardds_path.exists():
        print(f"\nâŒ é”™è¯¯: rewardDSç›®å½•ä¸å­˜åœ¨: {rewardds_path}")
        return 1
    
    pred_dir = rewardds_path / 'pred'
    gt_dir = rewardds_path / 'gt'
    
    if not pred_dir.exists():
        print(f"\nâŒ é”™è¯¯: é¢„æµ‹ç›®å½•ä¸å­˜åœ¨: {pred_dir}")
        return 1
    
    if not gt_dir.exists():
        print(f"\nâŒ é”™è¯¯: GTç›®å½•ä¸å­˜åœ¨: {gt_dir}")
        return 1
    
    # æŸ¥æ‰¾æ‰€æœ‰é¢„æµ‹æ–‡ä»¶
    pred_files = sorted(list(pred_dir.glob('*.nii.gz')))
    
    if not pred_files:
        print(f"\nâŒ é”™è¯¯: é¢„æµ‹ç›®å½•ä¸ºç©º: {pred_dir}")
        return 1
    
    print(f"\nğŸ“‚ rewardDSç›®å½•: {rewardds_path}")
    print(f"ğŸ“Š æ‰¾åˆ° {len(pred_files)} ä¸ªé¢„æµ‹æ–‡ä»¶")
    
    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
    all_metrics = {
        'accuracy': [],
        'precision': [],
        'sensitivity': [],
        'specificity': [],
        'f1': [],
        'iou': [],
        'dice': []
    }
    
    sample_details = []
    
    print("\n" + "="*80)
    print("å¼€å§‹è¯„ä¼°...")
    print("="*80 + "\n")
    
    # éå†æ‰€æœ‰é¢„æµ‹æ–‡ä»¶
    for pred_file in tqdm(pred_files, desc="è¯„ä¼°è¿›åº¦", ncols=80):
        try:
            # åŠ è½½é¢„æµ‹
            pred_img = nib.load(pred_file)
            pred_data = pred_img.get_fdata()
            
            # æ„é€ GTæ–‡ä»¶è·¯å¾„
            gt_file = gt_dir / pred_file.name
            
            if not gt_file.exists():
                if args.verbose:
                    print(f"âš ï¸  è·³è¿‡ {pred_file.name}: æ²¡æœ‰å¯¹åº”çš„GT")
                continue
            
            # åŠ è½½GT
            gt_img = nib.load(gt_file)
            gt_data = gt_img.get_fdata()
            
            # ç¡®ä¿å½¢çŠ¶åŒ¹é…
            if pred_data.shape != gt_data.shape:
                if args.verbose:
                    print(f"âš ï¸  è·³è¿‡ {pred_file.name}: å½¢çŠ¶ä¸åŒ¹é… {pred_data.shape} vs {gt_data.shape}")
                continue
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = calculate_metrics(pred_data, gt_data)
            
            # æ”¶é›†æŒ‡æ ‡
            for key in all_metrics.keys():
                if key in metrics:
                    all_metrics[key].append(metrics[key])
            
            # ä¿å­˜è¯¦ç»†ä¿¡æ¯
            sample_details.append({
                'filename': pred_file.name,
                'shape': pred_data.shape,
                **metrics
            })
            
            if args.verbose:
                print(f"âœ“ {pred_file.name}: Dice={metrics['dice']:.4f}, IoU={metrics['iou']:.4f}, F1={metrics['f1']:.4f}")
        
        except Exception as e:
            print(f"âŒ å¤„ç† {pred_file.name} æ—¶å‡ºé”™: {e}")
            continue
    
    # è®¡ç®—ç»Ÿè®¡ç»“æœ
    if not all_metrics['accuracy']:
        print("\nâŒ é”™è¯¯: æ²¡æœ‰æˆåŠŸè¯„ä¼°ä»»ä½•æ ·æœ¬")
        return 1
    
    print("\n" + "="*80)
    print(" "*30 + "è¯„ä¼°ç»“æœæ±‡æ€»")
    print("="*80)
    
    results_lines = []
    results_lines.append("\n" + "="*80)
    results_lines.append(" "*30 + "è¯„ä¼°ç»“æœæ±‡æ€»")
    results_lines.append("="*80)
    results_lines.append("")
    results_lines.append("æŒ‡æ ‡ç¼©å†™è¯´æ˜:")
    results_lines.append("  ACC  - Accuracy      (å‡†ç¡®ç‡)")
    results_lines.append("  PRE  - Precision     (ç²¾ç¡®ç‡)")
    results_lines.append("  SEN  - Sensitivity   (çµæ•åº¦/å¬å›ç‡)")
    results_lines.append("  SPE  - Specificity   (ç‰¹å¼‚åº¦)")
    results_lines.append("  F1   - F1 Score      (F1åˆ†æ•°)")
    results_lines.append("  IOU  - IoU/Jaccard   (äº¤å¹¶æ¯”)")
    results_lines.append("  DSC  - Dice          (Diceç³»æ•°)")
    results_lines.append("")
    results_lines.append("="*80)
    results_lines.append("")
    
    # è¡¨å¤´
    header = f"{'æŒ‡æ ‡':<15} {'å¹³å‡å€¼':>10} {'æ ‡å‡†å·®':>10} {'æœ€å°å€¼':>10} {'æœ€å¤§å€¼':>10}"
    results_lines.append(header)
    results_lines.append("-"*80)
    print(header)
    print("-"*80)
    
    # å®šä¹‰æŒ‡æ ‡æ˜¾ç¤ºé¡ºåºå’Œåç§°
    metrics_display = [
        ('accuracy', 'ACC'),
        ('precision', 'PRE'),
        ('sensitivity', 'SEN'),
        ('specificity', 'SPE'),
        ('f1', 'F1'),
        ('iou', 'IOU'),
        ('dice', 'DSC')
    ]
    
    for metric_key, metric_name in metrics_display:
        if metric_key in all_metrics and all_metrics[metric_key]:
            values = all_metrics[metric_key]
            mean_val = np.mean(values)
            std_val = np.std(values)
            min_val = np.min(values)
            max_val = np.max(values)
            
            line = f"{metric_name:<15} {mean_val:10.4f} {std_val:10.4f} {min_val:10.4f} {max_val:10.4f}"
            results_lines.append(line)
            print(line)
    
    results_lines.append("-"*80)
    results_lines.append(f"è¯„ä¼°æ ·æœ¬æ•°: {len(all_metrics['accuracy'])} ä¸ª")
    results_lines.append("="*80)
    
    print("-"*80)
    print(f"è¯„ä¼°æ ·æœ¬æ•°: {len(all_metrics['accuracy'])} ä¸ª")
    print("="*80)
    
    # æ·»åŠ æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
    if sample_details:
        results_lines.append("")
        results_lines.append("")
        results_lines.append("="*80)
        results_lines.append(" "*25 + "æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†æŒ‡æ ‡")
        results_lines.append("="*80)
        results_lines.append("")
        
        detail_header = f"{'æ–‡ä»¶å':<40} {'Dice':>8} {'IoU':>8} {'F1':>8} {'Acc':>8}"
        results_lines.append(detail_header)
        results_lines.append("-"*80)
        
        for detail in sample_details:
            detail_line = f"{detail['filename']:<40} {detail['dice']:8.4f} {detail['iou']:8.4f} {detail['f1']:8.4f} {detail['accuracy']:8.4f}"
            results_lines.append(detail_line)
    
    # ä¿å­˜ç»“æœ
    output_path = Path(args.output)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(results_lines))
    
    print(f"\nâœ“ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    # ç®€å•çš„æ€§èƒ½è¯„ä»·
    print("\n" + "="*80)
    print(" "*30 + "æ€§èƒ½è¯„ä»·")
    print("="*80)
    
    avg_dice = np.mean(all_metrics['dice'])
    avg_iou = np.mean(all_metrics['iou'])
    
    if avg_dice >= 0.8:
        quality = "ä¼˜ç§€ â­â­â­"
    elif avg_dice >= 0.7:
        quality = "è‰¯å¥½ â­â­"
    elif avg_dice >= 0.5:
        quality = "ä¸­ç­‰ â­"
    else:
        quality = "éœ€è¦æ”¹è¿›"
    
    print(f"\n  æ•´ä½“Dice Score: {avg_dice:.4f}")
    print(f"  æ•´ä½“IoU Score:  {avg_iou:.4f}")
    print(f"  æ¨¡å‹è´¨é‡è¯„çº§:   {quality}")
    print("\n" + "="*80)
    
    return 0

if __name__ == '__main__':
    import sys
    sys.exit(main())

