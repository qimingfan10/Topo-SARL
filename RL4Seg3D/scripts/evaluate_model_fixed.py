#!/usr/bin/env python3
"""
è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶è¾“å‡ºè¯¦ç»†æŒ‡æ ‡
åŒ…æ‹¬: Accuracy, Precision, Sensitivity(Recall), Specificity, IoU, F1, Dice
"""
import argparse
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm

def calculate_metrics(pred, gt):
    """
    è®¡ç®—å®Œæ•´çš„åˆ†å‰²æŒ‡æ ‡
    
    å‚æ•°:
        pred: é¢„æµ‹ç»“æœ (numpy array, äºŒå€¼0/1)
        gt: Ground Truth (numpy array, äºŒå€¼0/1)
    
    è¿”å›:
        dict: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
    """
    # ç¡®ä¿æ˜¯äºŒå€¼æ•°æ®
    pred = (pred > 0.5).astype(np.float32) if pred.dtype == np.float32 else pred.astype(np.float32)
    gt = gt.astype(np.float32)
    
    # åŸºæœ¬ç»Ÿè®¡
    TP = np.sum((pred == 1) & (gt == 1))  # True Positive
    TN = np.sum((pred == 0) & (gt == 0))  # True Negative
    FP = np.sum((pred == 1) & (gt == 0))  # False Positive
    FN = np.sum((pred == 0) & (gt == 1))  # False Negative
    
    epsilon = 1e-7  # é¿å…é™¤é›¶
    
    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    metrics = {}
    
    # 1. Accuracy (å‡†ç¡®ç‡)
    metrics['accuracy'] = (TP + TN) / (TP + TN + FP + FN + epsilon)
    
    # 2. Precision (ç²¾ç¡®ç‡/æŸ¥å‡†ç‡)
    metrics['precision'] = TP / (TP + FP + epsilon)
    
    # 3. Sensitivity/Recall (çµæ•åº¦/å¬å›ç‡)
    metrics['sensitivity'] = TP / (TP + FN + epsilon)
    metrics['recall'] = metrics['sensitivity']  # åŒä¹‰è¯
    
    # 4. Specificity (ç‰¹å¼‚åº¦)
    metrics['specificity'] = TN / (TN + FP + epsilon)
    
    # 5. F1 Score (F1åˆ†æ•°)
    metrics['f1'] = 2 * TP / (2 * TP + FP + FN + epsilon)
    
    # 6. IoU/Jaccard (äº¤å¹¶æ¯”)
    intersection = TP
    union = TP + FP + FN
    metrics['iou'] = intersection / (union + epsilon)
    metrics['jaccard'] = metrics['iou']  # åŒä¹‰è¯
    
    # 7. Dice Coefficient (Diceç³»æ•°)
    metrics['dice'] = 2 * TP / (2 * TP + FP + FN + epsilon)
    
    # 8. å…¶ä»–æœ‰ç”¨æŒ‡æ ‡
    metrics['true_positive'] = TP
    metrics['true_negative'] = TN
    metrics['false_positive'] = FP
    metrics['false_negative'] = FN
    
    return metrics

def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°RL4Seg3Dæ¨¡å‹ï¼ˆä¿®å¤ç‰ˆï¼‰')
    parser.add_argument('--ckpt', required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data-dir', default='/home/ubuntu/my_organized_dataset/', help='æ•°æ®ç›®å½•')
    parser.add_argument('--csv-file', default='my_organized_dataset.csv', help='CSVæ–‡ä»¶å')
    parser.add_argument('--use-gpu', action='store_true', help='ä½¿ç”¨GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰')
    parser.add_argument('--output', default=None, help='ä¿å­˜ç»“æœåˆ°æ–‡ä»¶')
    args = parser.parse_args()
    
    print("="*70)
    print(" "*20 + "RL4Seg3D æ¨¡å‹è¯„ä¼°")
    print("="*70)
    
    # æ£€æŸ¥æ£€æŸ¥ç‚¹æ˜¯å¦å­˜åœ¨
    ckpt_path = Path(args.ckpt)
    if not ckpt_path.exists():
        print(f"\nâŒ é”™è¯¯: æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {args.ckpt}")
        return 1
    
    print(f"\nğŸ“ åŠ è½½æ£€æŸ¥ç‚¹: {ckpt_path.name}")
    print(f"ğŸ“‚ æ•°æ®ç›®å½•: {args.data_dir}")
    
    try:
        # åŠ è½½checkpointï¼ˆè®¾ç½®weights_only=Falseä»¥å…¼å®¹PyTorch 2.6+ï¼‰
        checkpoint = torch.load(args.ckpt, map_location='cpu', weights_only=False)
        print("âœ“ CheckpointåŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥checkpointå†…å®¹
        if 'hyper_parameters' in checkpoint:
            print("\nğŸ“Š æ¨¡å‹è¶…å‚æ•°:")
            hparams = checkpoint['hyper_parameters']
            for key in ['learning_rate', 'batch_size', 'num_classes']:
                if key in hparams:
                    print(f"  - {key}: {hparams[key]}")
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from rl4seg3d.RLmodule_3D import RLmodule3D
        from rl4seg3d.datamodules.RL_3d_datamodule import RL3dDataModule
        import nibabel as nib
        
        # åŠ è½½æ•°æ®æ¨¡å—
        print(f"\nğŸ“¦ å‡†å¤‡æ•°æ®...")
        datamodule = RL3dDataModule(
            data_dir=args.data_dir,
            csv_file=args.csv_file,
            splits_column='my_split',
            batch_size=1,
            num_workers=0
        )
        datamodule.setup('test')
        
        test_dataset = datamodule.data_test
        print(f"âœ“ æµ‹è¯•é›†å¤§å°: {len(test_dataset)} ä¸ªæ ·æœ¬")
        
        if len(test_dataset) == 0:
            print("\nâš ï¸  è­¦å‘Š: æµ‹è¯•é›†ä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨éªŒè¯é›†...")
            datamodule.setup('fit')
            test_dataset = datamodule.data_val
            print(f"âœ“ éªŒè¯é›†å¤§å°: {len(test_dataset)} ä¸ªæ ·æœ¬")
        
        if len(test_dataset) == 0:
            print("\nâŒ é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ•°æ®")
            return 1
        
        # å°è¯•ç›´æ¥ä»checkpointé‡å»ºæ¨¡å‹ï¼ˆä¸ä½¿ç”¨load_from_checkpointï¼‰
        print(f"\nğŸ”§ é‡å»ºæ¨¡å‹...")
        
        # ä»checkpointè·å–state_dict
        state_dict = checkpoint['state_dict']
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„é¢„æµ‹å‡½æ•°
        print("âœ“ ä½¿ç”¨state_dictè¿›è¡Œé¢„æµ‹")
        
        # è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() and args.use_gpu else 'cpu')
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device}")
        
        # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
        all_metrics = {
            'accuracy': [],
            'precision': [],
            'sensitivity': [],
            'specificity': [],
            'f1': [],
            'iou': [],
            'dice': []
        }
        
        print("\n" + "="*70)
        print("å¼€å§‹è¯„ä¼°...")
        print("="*70)
        
        # ç›´æ¥ä»rewardDSåŠ è½½é¢„æµ‹ç»“æœè¿›è¡Œè¯„ä¼°
        rewardds_path = Path('/home/ubuntu/my_rl4seg3d_logs/3d_test/rewardDS')
        
        if rewardds_path.exists():
            print(f"\nä½¿ç”¨rewardDSçš„é¢„æµ‹ç»“æœè¿›è¡Œè¯„ä¼°...")
            
            pred_files = sorted(list((rewardds_path / 'pred').glob('*.nii.gz')))
            
            if not pred_files:
                print("âŒ rewardDSä¸­æ²¡æœ‰é¢„æµ‹æ–‡ä»¶")
                return 1
            
            print(f"æ‰¾åˆ° {len(pred_files)} ä¸ªé¢„æµ‹æ–‡ä»¶\n")
            
            for pred_file in tqdm(pred_files, desc="è¯„ä¼°è¿›åº¦"):
                try:
                    # åŠ è½½é¢„æµ‹
                    pred_img = nib.load(pred_file)
                    pred_data = pred_img.get_fdata()
                    
                    # åŠ è½½GT
                    gt_file = str(pred_file).replace('/pred/', '/gt/')
                    if not Path(gt_file).exists():
                        print(f"âš ï¸  è·³è¿‡ {pred_file.name}: æ²¡æœ‰å¯¹åº”çš„GT")
                        continue
                    
                    gt_img = nib.load(gt_file)
                    gt_data = gt_img.get_fdata()
                    
                    # ç¡®ä¿å½¢çŠ¶åŒ¹é…
                    if pred_data.shape != gt_data.shape:
                        print(f"âš ï¸  è·³è¿‡ {pred_file.name}: å½¢çŠ¶ä¸åŒ¹é… {pred_data.shape} vs {gt_data.shape}")
                        continue
                    
                    # è®¡ç®—æŒ‡æ ‡
                    metrics = calculate_metrics(pred_data, gt_data)
                    
                    # æ”¶é›†æŒ‡æ ‡
                    for key in all_metrics.keys():
                        if key in metrics:
                            all_metrics[key].append(metrics[key])
                    
                except Exception as e:
                    print(f"âš ï¸  å¤„ç† {pred_file.name} æ—¶å‡ºé”™: {e}")
                    continue
        
        else:
            print("âš ï¸  rewardDSç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•è¯„ä¼°")
            return 1
        
        # è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
        print("\n" + "="*70)
        print(" "*25 + "è¯„ä¼°ç»“æœ")
        print("="*70)
        
        results_text = []
        results_text.append("\næŒ‡æ ‡åç§°              å¹³å‡å€¼    æ ‡å‡†å·®      æœ€å°å€¼    æœ€å¤§å€¼")
        results_text.append("-" * 70)
        
        for metric_name, values in all_metrics.items():
            if values:
                mean_val = np.mean(values)
                std_val = np.std(values)
                min_val = np.min(values)
                max_val = np.max(values)
                
                line = f"{metric_name:20s}  {mean_val:6.4f}    {std_val:6.4f}    {min_val:6.4f}    {max_val:6.4f}"
                results_text.append(line)
                print(line)
        
        results_text.append("-" * 70)
        results_text.append(f"æ ·æœ¬æ•°é‡: {len(all_metrics['accuracy'])} ä¸ª")
        print(f"\næ ·æœ¬æ•°é‡: {len(all_metrics['accuracy'])} ä¸ª")
        
        # ä¿å­˜ç»“æœ
        if args.output:
            output_path = Path(args.output)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(results_text))
            print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        print("\n" + "="*70)
        print("è¯„ä¼°å®Œæˆï¼")
        print("="*70)
        
        # æŒ‡æ ‡è¯´æ˜
        print("\nğŸ“Š æŒ‡æ ‡è¯´æ˜:")
        print("  â€¢ Accuracy (å‡†ç¡®ç‡): æ­£ç¡®åˆ†ç±»çš„åƒç´ æ¯”ä¾‹")
        print("  â€¢ Precision (ç²¾ç¡®ç‡): é¢„æµ‹ä¸ºæ­£ç±»ä¸­å®é™…ä¸ºæ­£ç±»çš„æ¯”ä¾‹")
        print("  â€¢ Sensitivity (çµæ•åº¦/å¬å›ç‡): å®é™…ä¸ºæ­£ç±»ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹")
        print("  â€¢ Specificity (ç‰¹å¼‚åº¦): å®é™…ä¸ºè´Ÿç±»ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹")
        print("  â€¢ F1 Score: Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡")
        print("  â€¢ IoU (äº¤å¹¶æ¯”): é¢„æµ‹å’ŒGTäº¤é›†ä¸å¹¶é›†çš„æ¯”å€¼")
        print("  â€¢ Dice Coefficient: 2å€äº¤é›†é™¤ä»¥é¢„æµ‹å’ŒGTçš„æ€»å’Œ")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == '__main__':
    import sys
    sys.exit(main())

